{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Imported\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lead, to_date, lpad, min, max\n",
    "from pyspark.sql.window import Window\n",
    "sys.path.append('../src')\n",
    "from data import run, load_and_concat_csv, drop_na_columns, process_data, add_y_label,lppub_column_names, lppub_column_classes\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.DtypeWarning)\n",
    "print(\"Package Imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = '../data/raw/2016Q1.csv'\n",
    "# new_directory = '../data/processed'\n",
    "\n",
    "# df = run(file_path, new_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"MortgageDelinquency\").getOrCreate()\n",
    "\n",
    "# # Load the dataset (replace 'path_to_your_data.csv' with the actual path to your dataset)\n",
    "# df = spark.read.csv('../data/processed/2016Q1.csv', header=True, inferSchema=True)\n",
    "\n",
    "# # Convert 'ACT_PERIOD' from MMYYYY integer format to timestamp\n",
    "# df = df.withColumn('ACT_PERIOD', lpad(df['ACT_PERIOD'].cast('string'), 6, '0'))\n",
    "# df = df.withColumn('ACT_PERIOD', to_date(df['ACT_PERIOD'].cast('string'), 'MMyyyy'))\n",
    "# df = df.withColumn('default_status', col('default_status').cast('int'))\n",
    "\n",
    "# # Define a window specification to partition by LOAN_ID and order by ACT_PERIOD\n",
    "# window_spec = Window.partitionBy('LOAN_ID').orderBy('ACT_PERIOD')\n",
    "\n",
    "# # Create a column 'next_8_quarters' to look ahead for the next 8 quarters\n",
    "# # Use the lag function to check DLQ_STATUS for the next 8 quarters for each LOAN_ID\n",
    "# df = df.withColumn(\n",
    "#     'next_8_quarters_default',\n",
    "#     when(col('default_status') >= 3, 1).otherwise(0)\n",
    "# )\n",
    "\n",
    "# # Use the window spec to look at the next 8 quarters' default status\n",
    "# df = df.withColumn(\n",
    "#     'next_8_quarters_default',\n",
    "#     lead('next_8_quarters_default', 1).over(window_spec)\n",
    "# )\n",
    "\n",
    "# # Create the 'y_label' column based on next 8 quarters' default status\n",
    "# df = df.withColumn('y_label', when(col('next_8_quarters_default') == 1, 1).otherwise(0))\n",
    "\n",
    "# # Drop the intermediate 'next_8_quarters_default' column\n",
    "# df = df.drop('next_8_quarters_default')\n",
    "\n",
    "# # Show the results\n",
    "# df.select('LOAN_ID', 'ACT_PERIOD', 'default_status', 'y_label').show()\n",
    "\n",
    "# # Optionally, save the processed DataFrame to a new CSV or Parquet file\n",
    "# df_single = df.coalesce(1)\n",
    "# df_single.write.option(\"header\", \"true\").csv('../data/processed/2016Q1_ylabel.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mortgage_data(input_path: str, output_path: str):\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"MortgageDelinquency\").getOrCreate()\n",
    "\n",
    "    # Load the dataset\n",
    "    df = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "\n",
    "    # Convert 'ACT_PERIOD' from MMYYYY integer format to timestamp\n",
    "    df = df.withColumn('ACT_PERIOD', lpad(df['ACT_PERIOD'].cast('string'), 6, '0'))\n",
    "    df = df.withColumn('ACT_PERIOD', to_date(df['ACT_PERIOD'].cast('string'), 'MMyyyy'))\n",
    "    df = df.withColumn('default_status', col('default_status').cast('int'))\n",
    "\n",
    "    # Define a window specification to partition by LOAN_ID and order by ACT_PERIOD\n",
    "    window_spec = Window.partitionBy('LOAN_ID').orderBy('ACT_PERIOD')\n",
    "\n",
    "    # Create a column 'next_8_quarters' to look ahead for the next 8 quarters\n",
    "    df = df.withColumn(\n",
    "        'next_8_quarters_default',\n",
    "        when(col('default_status') >= 3, 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "    # Use the window spec to look at the next 8 quarters' default status\n",
    "    df = df.withColumn(\n",
    "        'next_8_quarters_default',\n",
    "        lead('next_8_quarters_default', 1).over(window_spec)\n",
    "    )\n",
    "\n",
    "    # Create the 'y_label' column based on next 8 quarters' default status\n",
    "    df = df.withColumn('y_label', when(col('next_8_quarters_default') == 1, 1).otherwise(0))\n",
    "\n",
    "    # Drop the intermediate 'next_8_quarters_default' column\n",
    "    df = df.drop('next_8_quarters_default')\n",
    "\n",
    "    # Convert Spark DataFrame to pandas DataFrame and save to CSV\n",
    "    df.coalesce(1).write.option(\"header\", \"true\").csv(output_path)\n",
    "\n",
    "    print(f\"Data processed and saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# input_dirc = '../data/raw/'\n",
    "# processed_dirc = '../data/processed/'\n",
    "# output_dirc = '../data/cleaned/'\n",
    "# file_name = '2015Q1.csv'\n",
    "# df = run(input_dirc+file_name, processed_dirc)\n",
    "# process_mortgage_data(processed_dirc + file_name, output_dirc + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dirc = '../data/raw/'\n",
    "# processed_dirc = '../data/processed/'\n",
    "# output_dirc = '../data/cleaned/'\n",
    "\n",
    "# # List of filenames from 2015Q2 to 2016Q4\n",
    "# file_names = ['2015Q2.csv', '2015Q3.csv', '2015Q4.csv', \n",
    "#               '2016Q1.csv', '2016Q2.csv', '2016Q3.csv', '2016Q4.csv']\n",
    "\n",
    "# # Loop through the list of files and process each one\n",
    "# for file_name in file_names:\n",
    "#     # Run for each file from raw to processed\n",
    "#     df = run(input_dirc + file_name, processed_dirc)\n",
    "    \n",
    "#     # Process the mortgage data and save to cleaned directory\n",
    "#     process_mortgage_data(processed_dirc + file_name, output_dirc + file_name)\n",
    "\n",
    "#     print(f\"Processing completed for {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV data...\n",
      "Processing data...\n",
      "Saving processed data...\n",
      "Processed data saved to: ../data/processed/2016Q4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/03 16:08:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "OpenJDK 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "OpenJDK 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=22168Kb max_used=22364Kb free=108903Kb\n",
      " bounds [0x0000000106984000, 0x0000000107fa4000, 0x000000010e984000]\n",
      " total_blobs=9188 nmethods=8264 adapters=836\n",
      " compilation: enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed and saved to ../data/cleaned/2016Q4.csv\n",
      "Processing completed for 2016Q4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "input_dirc = '../data/raw/'\n",
    "processed_dirc = '../data/processed/'\n",
    "output_dirc = '../data/cleaned/'\n",
    "\n",
    "# List of filenames from 2015Q2 to 2016Q4\n",
    "file_names = ['2016Q4.csv']\n",
    "\n",
    "# Loop through the list of files and process each one\n",
    "for file_name in file_names:\n",
    "    # Run for each file from raw to processed\n",
    "    df = run(input_dirc + file_name, processed_dirc)\n",
    "    \n",
    "    # Process the mortgage data and save to cleaned directory\n",
    "    process_mortgage_data(processed_dirc + file_name, output_dirc + file_name)\n",
    "\n",
    "    print(f\"Processing completed for {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MGT6785",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
