{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Imported\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lead, to_date, lpad, min, max\n",
    "from pyspark.sql.window import Window\n",
    "sys.path.append('../src')\n",
    "from data import run, load_and_concat_csv, drop_na_columns, process_data, add_y_label,lppub_column_names, lppub_column_classes\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.DtypeWarning)\n",
    "print(\"Package Imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = '../data/raw/2016Q1.csv'\n",
    "# new_directory = '../data/processed'\n",
    "\n",
    "# df = run(file_path, new_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 00:25:03 WARN Utils: Your hostname, Fengs-Laptop.local resolves to a loopback address: 127.0.0.1; using 10.91.162.124 instead (on interface en0)\n",
      "24/12/03 00:25:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/03 00:25:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "OpenJDK 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "OpenJDK 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n",
      "[Stage 2:>                                                         (0 + 8) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=21386Kb max_used=21436Kb free=109685Kb\n",
      " bounds [0x000000010a984000, 0x000000010be94000, 0x0000000112984000]\n",
      " total_blobs=8879 nmethods=7964 adapters=828\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------+-------+\n",
      "|     LOAN_ID|ACT_PERIOD|default_status|y_label|\n",
      "+------------+----------+--------------+-------+\n",
      "|100009919815|2016-01-01|             0|      0|\n",
      "|100009919815|2016-02-01|             0|      0|\n",
      "|100009919815|2016-03-01|             0|      0|\n",
      "|100009919815|2016-04-01|             0|      0|\n",
      "|100009919815|2016-05-01|             0|      0|\n",
      "|100009919815|2016-06-01|             0|      0|\n",
      "|100009919815|2016-07-01|             0|      0|\n",
      "|100009919815|2016-08-01|             0|      0|\n",
      "|100009919815|2016-09-01|             0|      0|\n",
      "|100009919815|2016-10-01|             0|      0|\n",
      "|100009919815|2016-11-01|             0|      0|\n",
      "|100009919815|2016-12-01|             0|      0|\n",
      "|100009919815|2017-01-01|             0|      0|\n",
      "|100009919815|2017-02-01|             0|      0|\n",
      "|100009919815|2017-03-01|             0|      0|\n",
      "|100009919815|2017-04-01|             0|      0|\n",
      "|100009919815|2017-05-01|             0|      0|\n",
      "|100009919815|2017-06-01|             0|      0|\n",
      "|100009919815|2017-07-01|             0|      0|\n",
      "|100009919815|2017-08-01|             0|      0|\n",
      "+------------+----------+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"MortgageDelinquency\").getOrCreate()\n",
    "\n",
    "# Load the dataset (replace 'path_to_your_data.csv' with the actual path to your dataset)\n",
    "df = spark.read.csv('../data/processed/2016Q1.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Convert 'ACT_PERIOD' from MMYYYY integer format to timestamp\n",
    "df = df.withColumn('ACT_PERIOD', lpad(df['ACT_PERIOD'].cast('string'), 6, '0'))\n",
    "df = df.withColumn('ACT_PERIOD', to_date(df['ACT_PERIOD'].cast('string'), 'MMyyyy'))\n",
    "df = df.withColumn('default_status', col('default_status').cast('int'))\n",
    "\n",
    "# Define a window specification to partition by LOAN_ID and order by ACT_PERIOD\n",
    "window_spec = Window.partitionBy('LOAN_ID').orderBy('ACT_PERIOD')\n",
    "\n",
    "# Create a column 'next_8_quarters' to look ahead for the next 8 quarters\n",
    "# Use the lag function to check DLQ_STATUS for the next 8 quarters for each LOAN_ID\n",
    "df = df.withColumn(\n",
    "    'next_8_quarters_default',\n",
    "    when(col('default_status') >= 3, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Use the window spec to look at the next 8 quarters' default status\n",
    "df = df.withColumn(\n",
    "    'next_8_quarters_default',\n",
    "    lead('next_8_quarters_default', 1).over(window_spec)\n",
    ")\n",
    "\n",
    "# Create the 'y_label' column based on next 8 quarters' default status\n",
    "df = df.withColumn('y_label', when(col('next_8_quarters_default') == 1, 1).otherwise(0))\n",
    "\n",
    "# Drop the intermediate 'next_8_quarters_default' column\n",
    "df = df.drop('next_8_quarters_default')\n",
    "\n",
    "# Show the results\n",
    "df.select('LOAN_ID', 'ACT_PERIOD', 'default_status', 'y_label').show()\n",
    "\n",
    "# Optionally, save the processed DataFrame to a new CSV or Parquet file\n",
    "df_single = df.coalesce(1)\n",
    "# df_single.write.option(\"header\", \"true\").csv('../data/processed/2016Q1_ylabel.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Output column seller_type_index already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m test_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mACT_PERIOD\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2018-12-30\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m categorical_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseller_type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mservicer_type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_type\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpurpose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproperty_type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moccupancy_status\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayment_deferral\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m     ]\n\u001b[0;32m---> 14\u001b[0m indexers \u001b[38;5;241m=\u001b[39m [\u001b[43mStringIndexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m categorical_columns]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m indexer \u001b[38;5;129;01min\u001b[39;00m indexers:\n\u001b[1;32m     16\u001b[0m     df \u001b[38;5;241m=\u001b[39m indexer\u001b[38;5;241m.\u001b[39mtransform(df)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MGT6785/lib/python3.12/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MGT6785/lib/python3.12/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MGT6785/lib/python3.12/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MGT6785/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MGT6785/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Output column seller_type_index already exists."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "train_df = df.filter(df['ACT_PERIOD'] <= '2016-12-30')\n",
    "test_df = df.filter(df['ACT_PERIOD'] > '2018-12-30')\n",
    "categorical_columns = [\n",
    "    'seller_type', 'servicer_type', 'channel_type',\n",
    "    'purpose', 'property_type', 'occupancy_status', 'state', \n",
    "    'default_status', 'high_balance_loan_indicator', 'mod_indicator', \n",
    "    'homeready_indicator', 'relocation_mortgage_indicator', 'htlv_indicator', \n",
    "    'payment_deferral'\n",
    "    ]\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + '_index').fit(df) for col in categorical_columns]\n",
    "for indexer in indexers:\n",
    "    df = indexer.transform(df)\n",
    "    train_df = indexer.transform(train_df)\n",
    "    test_df = indexer.transform(test_df)\n",
    "\n",
    "# Assemble features (you can choose the features you want to use)\n",
    "feature_columns = [\n",
    "    'adjusted_remaining_time',  # Numeric column\n",
    "    'num_borrowers', # Numeric column\n",
    "    'seller_type_index', 'servicer_type_index', 'channel_type_index', \n",
    "    'purpose_index', 'property_type_index', 'occupancy_status_index', \n",
    "    'state_index', 'high_balance_loan_indicator_index', 'mod_indicator_index', \n",
    "    'homeready_indicator_index', 'relocation_mortgage_indicator_index', \n",
    "    'htlv_indicator_index', 'payment_deferral_index'\n",
    "    ]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features', handleInvalid='skip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 00:34:09 ERROR Instrumentation: java.lang.IllegalArgumentException: requirement failed: Binomial family only supports 1 or 2 outcome classes but found 12.\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.checkMultinomial(LogisticRegression.scala:695)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:554)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Binomial family only supports 1 or 2 outcome classes but found 12.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m lr \u001b[38;5;241m=\u001b[39m LogisticRegression(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault_status\u001b[39m\u001b[38;5;124m'\u001b[39m, featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, family\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinomial\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Train the model on the training data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Make predictions on the test data\u001b[39;00m\n\u001b[1;32m     11\u001b[0m predictions \u001b[38;5;241m=\u001b[39m lr_model\u001b[38;5;241m.\u001b[39mtransform(test_df)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MGT6785/lib/python3.12/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MGT6785/lib/python3.12/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MGT6785/lib/python3.12/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MGT6785/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/MGT6785/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Binomial family only supports 1 or 2 outcome classes but found 12."
     ]
    }
   ],
   "source": [
    "train_df = assembler.transform(train_df)\n",
    "test_df = assembler.transform(test_df)\n",
    "\n",
    "# Initialize LogisticRegression model\n",
    "lr = LogisticRegression(labelCol='default_status', featuresCol='features', family='binomial')\n",
    "\n",
    "# Train the model on the training data\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='default_status')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string_columns = final_df.select_dtypes(include='string').columns\n",
    "# unique_values = {col: final_df[col].unique() for col in string_columns}\n",
    "# for col, values in unique_values.items():\n",
    "#     print(f\"Unique values in column '{col}': {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_servicer_type(seller_name):\n",
    "    # Convert to lowercase for consistent matching\n",
    "    seller_name = seller_name.lower()\n",
    "    \n",
    "    # Define keywords for each category\n",
    "    bank_keywords = [\n",
    "        'bank', 'national association', 'credit union', 'fifth third', \n",
    "        'pnc', 'citizens bank', 'wells fargo', 'regions bank', \n",
    "        'suntrust', 'truist', 'jpmorgan', 'citi'\n",
    "    ]\n",
    "    \n",
    "    mortgage_company_keywords = [\n",
    "        'mortgage', 'lending', 'loan', 'servicing', 'financial', \n",
    "        'homeloans', 'loandepot', 'pennymac', 'roundpoint', \n",
    "        'freedom', 'quicken', 'amerihome', 'guild', 'caliber'\n",
    "    ]\n",
    "    \n",
    "    # Check for keywords in the seller name\n",
    "    if any(keyword in seller_name for keyword in bank_keywords):\n",
    "        return 'Bank'\n",
    "    elif any(keyword in seller_name for keyword in mortgage_company_keywords):\n",
    "        return 'Mortgage Company'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the classification function to the SELLER column\n",
    "final_df['servicer_type'] = final_df['SELLER'].apply(classify_servicer_type)\n",
    "print(final_df['servicer_type'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df['CHANNEL'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_map = {'C': 0, 'B': 1, 'R': 2}\n",
    "final_df['CHANNEL1'] = final_df['CHANNEL'].map(channel_map)\n",
    "print(final_df['CHANNEL1'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## firstpayment date - orginal date = time_to_first_payment\n",
    "## ADJ Rem month / ORIGNAL Term\n",
    "## Num of borrow stirng to int\n",
    "## First Flag N: 0 Y: 1\n",
    "## Purpose: P: 0  C R U: 1\n",
    "## PROP: SF: 0, PU: 1, CO: 2, MH: 3, CP: 4\n",
    "## Occupancy Status: U: 0 P: 1 I: 2 S: 3\n",
    "## State: Alphabet sorted \n",
    "## MSA/ZIP leave for comment\n",
    "## Product abandon\n",
    "## \"DLQ_STATUS\" (string to int)\n",
    "## MOD FLAG remove NA (N: 0 Y: 1)\n",
    "## SERV_IND  (N: 0 Y: 1)\n",
    "## HOMEREADY_PROGRAM_INDICATOR: 7: 0 F: 1 H:2\n",
    "## RELOCATION_MORTGAGE_INDICATOR N: 0 Y:1\n",
    "## HIGH_BALANCE_LOAN_INDICATOR N: 0 Y:1\n",
    "## ACT Period -> date time format YYYY MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['adjusted_remaining_ratio'] = final_df['ADJ_REM_MONTHS'] / final_df['ORIG_TERM']\n",
    "print(final_df['adjusted_remaining_ratio'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['NUM_BO'] = pd.to_numeric(final_df['NUM_BO'], errors='coerce')\n",
    "print(final_df['NUM_BO'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[''] = pd.to_numeric(final_df['NUM_BO'], errors='coerce')\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['purpose'] = final_df['PURPOSE'].map({'P': 0, 'C': 1, 'R': 1, 'U': 1})\n",
    "print(final_df['purpose'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['high_balance_loan_indicator'] = final_df['HIGH_BALANCE_LOAN_INDICATOR'].map({'N': 0, 'Y': 1})\n",
    "print(final_df['high_balance_loan_indicator'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_map = {'SF': 0, 'PU': 1, 'CO': 2, 'MH': 3, 'CP': 4}\n",
    "final_df['PROP'] = final_df['PROP'].map(prop_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_map = {'U': 0, 'P': 1, 'I': 2, 'S': 3}\n",
    "final_df['OCC_STAT'] = final_df['OCC_STAT'].map(occupancy_map)\n",
    "print(final_df['OCC_STAT'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['SERV_IND'] = final_df['SERV_IND'].map({'N': 0, 'Y': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['MOD_FLAG'] = final_df['MOD_FLAG'].map({'N': 0, 'Y': 1}).fillna(0)\n",
    "print(final_df['MOD_FLAG'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ORIG_DATE FIRST_PAY MATR_DT not in datetime format right now, need to change later to\n",
    "\n",
    "\n",
    "channel_map = {'C': 0, 'B': 1, 'R': 2}\n",
    "final_df['CHANNEL'] = final_df['CHANNEL'].map(channel_map)\n",
    "\n",
    "\n",
    "# 1. Calculate `time_to_first_payment` as the difference between `firstpayment` and `origination date`\n",
    "# need do it later \n",
    "\n",
    "# 2. Calculate `ADJ Rem month / ORIG_TERM`\n",
    "final_df['adjusted_remaining_ratio'] = final_df['ADJ_REM_MONTHS'] / final_df['ORIG_TERM']\n",
    "\n",
    "# 3. Convert `NUM_BO` (number of borrowers) from string to integer\n",
    "final_df['NUM_BO'] = pd.to_numeric(final_df['NUM_BO'], errors='coerce')\n",
    "\n",
    "# 4. Convert `FIRST_FLAG` (N: 0, Y: 1)\n",
    "final_df['FIRST_FLAG'] = final_df['FIRST_FLAG'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "# 5. Convert `PURPOSE` (P: 0, C/R/U: 1)\n",
    "final_df['PURPOSE'] = final_df['PURPOSE'].map({'P': 0, 'C': 1, 'R': 1, 'U': 1})\n",
    "\n",
    "# 6. Convert `PROP` (SF: 0, PU: 1, CO: 2, MH: 3, CP: 4)\n",
    "prop_map = {'SF': 0, 'PU': 1, 'CO': 2, 'MH': 3, 'CP': 4}\n",
    "final_df['PROP'] = final_df['PROP'].map(prop_map)\n",
    "\n",
    "# 7. Convert `OCCUPANCY_STATUS` (U: 0, P: 1, I: 2, S: 3)\n",
    "occupancy_map = {'U': 0, 'P': 1, 'I': 2, 'S': 3}\n",
    "final_df['OCC_STAT'] = final_df['OCC_STAT'].map(occupancy_map)\n",
    "\n",
    "# 8. Convert `STATE` to integers by alphabetically sorting unique values and assigning them numbers\n",
    "# Provided list of states in the dataset\n",
    "states = ['GA', 'KS', 'IL', 'IN', 'TX', 'UT', 'MO', 'IA', 'OR', 'DE', 'CA', 'MI', 'KY',\n",
    "          'CO', 'NY', 'PA', 'WI', 'WA', 'VA', 'AZ', 'MD', 'TN', 'MA', 'OH', 'SC', 'AK',\n",
    "          'AL', 'LA', 'MN', 'NC', 'AR', 'MS', 'OK', 'NE', 'NJ', 'ID', 'FL', 'ND', 'NV',\n",
    "          'NM', 'CT', 'VT', 'WV', 'DC', 'ME', 'SD', 'NH', 'MT', 'HI', 'PR', 'RI', 'WY',\n",
    "          'VI', 'GU']\n",
    "\n",
    "# Sort states alphabetically and assign rank values starting from 1\n",
    "state_mapping = {state: idx + 1 for idx, state in enumerate(sorted(states))}\n",
    "final_df['STATE'] = final_df['STATE'].map(state_mapping)\n",
    "\n",
    "\n",
    "# 9. Leave `MSA` and `ZIP` columns as-is for now, as instructed, need further exploration\n",
    "\n",
    "# 10. Drop the `PRODUCT` column\n",
    "final_df.drop(columns=['PRODUCT'], inplace=True)\n",
    "\n",
    "# 11. Convert `DLQ_STATUS` from string to integer, treating non-numeric as NaN\n",
    "final_df['DLQ_STATUS'] = pd.to_numeric(final_df['DLQ_STATUS'], errors='coerce')\n",
    "\n",
    "# 12. Convert `MOD_FLAG` (N: 0, Y: 1), removing rows where `MOD_FLAG` is NA\n",
    "final_df = final_df.dropna(subset=['MOD_FLAG'])\n",
    "final_df['MOD_FLAG'] = final_df['MOD_FLAG'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "# 13. Convert `SERV_IND` (N: 0, Y: 1)\n",
    "final_df['SERV_IND'] = final_df['SERV_IND'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "# 14. Convert `HOMEREADY_PROGRAM_INDICATOR` (7: 0, F: 1, H: 2)\n",
    "final_df['HOMEREADY_PROGRAM_INDICATOR'] = final_df['HOMEREADY_PROGRAM_INDICATOR'].map({'7': 0, 'F': 1, 'H': 2})\n",
    "\n",
    "# 15. Convert `RELOCATION_MORTGAGE_INDICATOR` (N: 0, Y: 1)\n",
    "final_df['RELOCATION_MORTGAGE_INDICATOR'] = final_df['RELOCATION_MORTGAGE_INDICATOR'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "# 16. Convert `HIGH_BALANCE_LOAN_INDICATOR` (N: 0, Y: 1)\n",
    "final_df['HIGH_BALANCE_LOAN_INDICATOR'] = final_df['HIGH_BALANCE_LOAN_INDICATOR'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PPMT_FLG'] = df['PPMT_FLG'].map({'N': 0, 'Y': 1})\n",
    "print(df['interst_only_loan'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prepayment_penalty'] = df['PPMT_FLG'].map({'N': 0, 'Y': 1})\n",
    "print(df['prepayment_penalty'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['interst_only_loan'] = df['IO'].map({'N': 0, 'Y': 1})\n",
    "print(df['interst_only_loan'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17 HIGH_LOAN_TO_VALUE_HLTV_REFINANCE_OPTION_INDICATOR\n",
    "final_df['HIGH_LOAN_TO_VALUE_HLTV_REFINANCE_OPTION_INDICATOR'] = final_df['HIGH_LOAN_TO_VALUE_HLTV_REFINANCE_OPTION_INDICATOR'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "#18 PAYMENT_DEFERRAL_MOD_EVENT_FLAG\n",
    "final_df['PAYMENT_DEFERRAL_MOD_EVENT_FLAG'] = final_df['PAYMENT_DEFERRAL_MOD_EVENT_FLAG'].map({'N': 0, 'Y': 1,'7': 2})\n",
    "# Checking the data types and first few rows to confirm transformations\n",
    "\n",
    "#19 PPMT_FLG\n",
    "final_df['PPMT_FLG'] = final_df['PPMT_FLG'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "#20 IO\n",
    "final_df['IO'] = final_df['IO'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "# Map classification categories to integers (Bank: 0, Mortgage Company: 1, Investor: 2, Other: 3)\n",
    "seller_type_mapping = {'Bank': 0, 'Mortgage Company': 1, 'Other': 2, 'Investor': 3}\n",
    "final_df['seller_type'] = final_df['seller_type'].map(seller_type_mapping)\n",
    "\n",
    "\n",
    "Service_type_mapping = {'Bank': 0, 'Mortgage Company': 1, 'Other': 2}\n",
    "final_df['servicer_type'] = final_df['servicer_type'].map(Service_type_mapping)\n",
    "\n",
    "\n",
    "\n",
    "print(final_df.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NaN values in each column in the dataset\n",
    "nan_counts = final_df.isna().sum()\n",
    "\n",
    "# Display columns with NaN values and their counts\n",
    "nan_counts[nan_counts > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in the specified columns and save the cleaned DataFrame to a CSV file\n",
    "\n",
    "# Specify columns with NaN values to drop\n",
    "columns_with_nan = [\n",
    "    'SERVICER', 'LOAN_AGE', 'REM_MONTHS', 'ADJ_REM_MONTHS', 'MATR_DT',\n",
    "    'DTI', 'CSCORE_B', 'ZIP', 'PPMT_FLG', 'IO', 'DLQ_STATUS', \n",
    "    'SERV_IND', 'adjusted_remaining_ratio'\n",
    "]\n",
    "\n",
    "# Drop rows with NaN values in these columns\n",
    "cleaned_df = final_df.dropna(subset=columns_with_nan)\n",
    "final_df = final_df.drop(columns=['SELLER','SERVICER' 'ORIG_DATE','FIRST_PAY','MSA','ZIP'], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"../data/final_16Q1.csv\"\n",
    "final_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hltv_INDICATOR'] = df['HIGH_LOAN_TO_VALUE_HLTV_REFINANCE_OPTION_INDICATOR'].map({'N': 0, 'Y': 1})\n",
    "print(df['hltv_INDICATOR'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MGT6785",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
