{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Imported\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lead, to_date, lpad, min, max\n",
    "from pyspark.sql.window import Window\n",
    "sys.path.append('../src')\n",
    "from data import run, load_and_concat_csv, drop_na_columns, process_data, add_y_label,lppub_column_names, lppub_column_classes\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.DtypeWarning)\n",
    "print(\"Package Imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = '../data/raw/2016Q1.csv'\n",
    "# new_directory = '../data/processed'\n",
    "\n",
    "# df = run(file_path, new_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 00:37:09 WARN Utils: Your hostname, Fengs-Laptop.local resolves to a loopback address: 127.0.0.1; using 10.91.162.124 instead (on interface en0)\n",
      "24/12/03 00:37:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/03 00:37:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "OpenJDK 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "OpenJDK 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=21576Kb max_used=21672Kb free=109495Kb\n",
      " bounds [0x0000000108984000, 0x0000000109ed4000, 0x0000000110984000]\n",
      " total_blobs=8857 nmethods=7941 adapters=828\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------+-------+\n",
      "|     LOAN_ID|ACT_PERIOD|default_status|y_label|\n",
      "+------------+----------+--------------+-------+\n",
      "|100009919815|2016-01-01|             0|      0|\n",
      "|100009919815|2016-02-01|             0|      0|\n",
      "|100009919815|2016-03-01|             0|      0|\n",
      "|100009919815|2016-04-01|             0|      0|\n",
      "|100009919815|2016-05-01|             0|      0|\n",
      "|100009919815|2016-06-01|             0|      0|\n",
      "|100009919815|2016-07-01|             0|      0|\n",
      "|100009919815|2016-08-01|             0|      0|\n",
      "|100009919815|2016-09-01|             0|      0|\n",
      "|100009919815|2016-10-01|             0|      0|\n",
      "|100009919815|2016-11-01|             0|      0|\n",
      "|100009919815|2016-12-01|             0|      0|\n",
      "|100009919815|2017-01-01|             0|      0|\n",
      "|100009919815|2017-02-01|             0|      0|\n",
      "|100009919815|2017-03-01|             0|      0|\n",
      "|100009919815|2017-04-01|             0|      0|\n",
      "|100009919815|2017-05-01|             0|      0|\n",
      "|100009919815|2017-06-01|             0|      0|\n",
      "|100009919815|2017-07-01|             0|      0|\n",
      "|100009919815|2017-08-01|             0|      0|\n",
      "+------------+----------+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"MortgageDelinquency\").getOrCreate()\n",
    "\n",
    "# Load the dataset (replace 'path_to_your_data.csv' with the actual path to your dataset)\n",
    "df = spark.read.csv('../data/processed/2016Q1.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Convert 'ACT_PERIOD' from MMYYYY integer format to timestamp\n",
    "df = df.withColumn('ACT_PERIOD', lpad(df['ACT_PERIOD'].cast('string'), 6, '0'))\n",
    "df = df.withColumn('ACT_PERIOD', to_date(df['ACT_PERIOD'].cast('string'), 'MMyyyy'))\n",
    "df = df.withColumn('default_status', col('default_status').cast('int'))\n",
    "\n",
    "# Define a window specification to partition by LOAN_ID and order by ACT_PERIOD\n",
    "window_spec = Window.partitionBy('LOAN_ID').orderBy('ACT_PERIOD')\n",
    "\n",
    "# Create a column 'next_8_quarters' to look ahead for the next 8 quarters\n",
    "# Use the lag function to check DLQ_STATUS for the next 8 quarters for each LOAN_ID\n",
    "df = df.withColumn(\n",
    "    'next_8_quarters_default',\n",
    "    when(col('default_status') >= 3, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Use the window spec to look at the next 8 quarters' default status\n",
    "df = df.withColumn(\n",
    "    'next_8_quarters_default',\n",
    "    lead('next_8_quarters_default', 1).over(window_spec)\n",
    ")\n",
    "\n",
    "# Create the 'y_label' column based on next 8 quarters' default status\n",
    "df = df.withColumn('y_label', when(col('next_8_quarters_default') == 1, 1).otherwise(0))\n",
    "\n",
    "# Drop the intermediate 'next_8_quarters_default' column\n",
    "df = df.drop('next_8_quarters_default')\n",
    "\n",
    "# Show the results\n",
    "df.select('LOAN_ID', 'ACT_PERIOD', 'default_status', 'y_label').show()\n",
    "\n",
    "# Optionally, save the processed DataFrame to a new CSV or Parquet file\n",
    "df_single = df.coalesce(1)\n",
    "# df_single.write.option(\"header\", \"true\").csv('../data/processed/2016Q1_ylabel.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 00:40:11 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/12/03 00:40:43 WARN MemoryStore: Not enough space to cache rdd_157_0 in memory! (computed 17.0 MiB so far)\n",
      "24/12/03 00:40:43 WARN BlockManager: Persisting block rdd_157_0 to disk instead.\n",
      "24/12/03 00:40:44 WARN MemoryStore: Not enough space to cache rdd_157_5 in memory! (computed 17.0 MiB so far)\n",
      "24/12/03 00:40:44 WARN BlockManager: Persisting block rdd_157_5 to disk instead.\n",
      "24/12/03 00:40:44 WARN MemoryStore: Not enough space to cache rdd_157_1 in memory! (computed 17.0 MiB so far)\n",
      "24/12/03 00:40:44 WARN BlockManager: Persisting block rdd_157_1 to disk instead.\n",
      "24/12/03 00:40:54 WARN MemoryStore: Not enough space to cache rdd_157_4 in memory! (computed 33.0 MiB so far)\n",
      "24/12/03 00:40:54 WARN BlockManager: Persisting block rdd_157_4 to disk instead.\n",
      "24/12/03 00:40:54 WARN MemoryStore: Not enough space to cache rdd_157_6 in memory! (computed 33.0 MiB so far)\n",
      "24/12/03 00:40:54 WARN BlockManager: Persisting block rdd_157_6 to disk instead.\n",
      "24/12/03 00:40:55 WARN MemoryStore: Not enough space to cache rdd_157_3 in memory! (computed 33.0 MiB so far)\n",
      "24/12/03 00:40:55 WARN BlockManager: Persisting block rdd_157_3 to disk instead.\n",
      "[Stage 114:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6864054922399269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "train_df = df.filter(df['ACT_PERIOD'] <= '2016-12-30')\n",
    "test_df = df.filter(df['ACT_PERIOD'] > '2018-12-30')\n",
    "categorical_columns = [\n",
    "    'seller_type', 'servicer_type', 'channel_type',\n",
    "    'purpose', 'property_type', 'occupancy_status', 'state', \n",
    "    'default_status', 'high_balance_loan_indicator', 'mod_indicator', \n",
    "    'homeready_indicator', 'relocation_mortgage_indicator', 'htlv_indicator', \n",
    "    'payment_deferral'\n",
    "    ]\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + '_index').fit(df) for col in categorical_columns]\n",
    "for indexer in indexers:\n",
    "    df = indexer.transform(df)\n",
    "    train_df = indexer.transform(train_df)\n",
    "    test_df = indexer.transform(test_df)\n",
    "\n",
    "# Assemble features (you can choose the features you want to use)\n",
    "feature_columns = [\n",
    "    'adjusted_remaining_time',  # Numeric column\n",
    "    'num_borrowers', # Numeric column\n",
    "    'seller_type_index', 'servicer_type_index', 'channel_type_index', \n",
    "    'purpose_index', 'property_type_index', 'occupancy_status_index', \n",
    "    'state_index', 'high_balance_loan_indicator_index', 'mod_indicator_index', \n",
    "    'homeready_indicator_index', 'relocation_mortgage_indicator_index', \n",
    "    'htlv_indicator_index', 'payment_deferral_index'\n",
    "    ]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features', handleInvalid='skip')\n",
    "train_df = assembler.transform(train_df)\n",
    "test_df = assembler.transform(test_df)\n",
    "\n",
    "# Initialize LogisticRegression model\n",
    "lr = LogisticRegression(labelCol='y_label', featuresCol='features', family='binomial')\n",
    "\n",
    "# Train the model on the training data\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='y_label')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string_columns = final_df.select_dtypes(include='string').columns\n",
    "# unique_values = {col: final_df[col].unique() for col in string_columns}\n",
    "# for col, values in unique_values.items():\n",
    "#     print(f\"Unique values in column '{col}': {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOther\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Apply the classification function to the SELLER column\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m final_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mservicer_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSELLER\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(classify_servicer_type)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mservicer_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_df' is not defined"
     ]
    }
   ],
   "source": [
    "def classify_servicer_type(seller_name):\n",
    "    # Convert to lowercase for consistent matching\n",
    "    seller_name = seller_name.lower()\n",
    "    \n",
    "    # Define keywords for each category\n",
    "    bank_keywords = [\n",
    "        'bank', 'national association', 'credit union', 'fifth third', \n",
    "        'pnc', 'citizens bank', 'wells fargo', 'regions bank', \n",
    "        'suntrust', 'truist', 'jpmorgan', 'citi'\n",
    "    ]\n",
    "    \n",
    "    mortgage_company_keywords = [\n",
    "        'mortgage', 'lending', 'loan', 'servicing', 'financial', \n",
    "        'homeloans', 'loandepot', 'pennymac', 'roundpoint', \n",
    "        'freedom', 'quicken', 'amerihome', 'guild', 'caliber'\n",
    "    ]\n",
    "    \n",
    "    # Check for keywords in the seller name\n",
    "    if any(keyword in seller_name for keyword in bank_keywords):\n",
    "        return 'Bank'\n",
    "    elif any(keyword in seller_name for keyword in mortgage_company_keywords):\n",
    "        return 'Mortgage Company'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the classification function to the SELLER column\n",
    "final_df['servicer_type'] = final_df['SELLER'].apply(classify_servicer_type)\n",
    "print(final_df['servicer_type'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df['CHANNEL'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_map = {'C': 0, 'B': 1, 'R': 2}\n",
    "final_df['CHANNEL1'] = final_df['CHANNEL'].map(channel_map)\n",
    "print(final_df['CHANNEL1'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## firstpayment date - orginal date = time_to_first_payment\n",
    "## ADJ Rem month / ORIGNAL Term\n",
    "## Num of borrow stirng to int\n",
    "## First Flag N: 0 Y: 1\n",
    "## Purpose: P: 0  C R U: 1\n",
    "## PROP: SF: 0, PU: 1, CO: 2, MH: 3, CP: 4\n",
    "## Occupancy Status: U: 0 P: 1 I: 2 S: 3\n",
    "## State: Alphabet sorted \n",
    "## MSA/ZIP leave for comment\n",
    "## Product abandon\n",
    "## \"DLQ_STATUS\" (string to int)\n",
    "## MOD FLAG remove NA (N: 0 Y: 1)\n",
    "## SERV_IND  (N: 0 Y: 1)\n",
    "## HOMEREADY_PROGRAM_INDICATOR: 7: 0 F: 1 H:2\n",
    "## RELOCATION_MORTGAGE_INDICATOR N: 0 Y:1\n",
    "## HIGH_BALANCE_LOAN_INDICATOR N: 0 Y:1\n",
    "## ACT Period -> date time format YYYY MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['adjusted_remaining_ratio'] = final_df['ADJ_REM_MONTHS'] / final_df['ORIG_TERM']\n",
    "print(final_df['adjusted_remaining_ratio'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['NUM_BO'] = pd.to_numeric(final_df['NUM_BO'], errors='coerce')\n",
    "print(final_df['NUM_BO'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[''] = pd.to_numeric(final_df['NUM_BO'], errors='coerce')\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['purpose'] = final_df['PURPOSE'].map({'P': 0, 'C': 1, 'R': 1, 'U': 1})\n",
    "print(final_df['purpose'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['high_balance_loan_indicator'] = final_df['HIGH_BALANCE_LOAN_INDICATOR'].map({'N': 0, 'Y': 1})\n",
    "print(final_df['high_balance_loan_indicator'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_map = {'SF': 0, 'PU': 1, 'CO': 2, 'MH': 3, 'CP': 4}\n",
    "final_df['PROP'] = final_df['PROP'].map(prop_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_map = {'U': 0, 'P': 1, 'I': 2, 'S': 3}\n",
    "final_df['OCC_STAT'] = final_df['OCC_STAT'].map(occupancy_map)\n",
    "print(final_df['OCC_STAT'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['SERV_IND'] = final_df['SERV_IND'].map({'N': 0, 'Y': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['MOD_FLAG'] = final_df['MOD_FLAG'].map({'N': 0, 'Y': 1}).fillna(0)\n",
    "print(final_df['MOD_FLAG'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ORIG_DATE FIRST_PAY MATR_DT not in datetime format right now, need to change later to\n",
    "\n",
    "\n",
    "channel_map = {'C': 0, 'B': 1, 'R': 2}\n",
    "final_df['CHANNEL'] = final_df['CHANNEL'].map(channel_map)\n",
    "\n",
    "\n",
    "# 1. Calculate `time_to_first_payment` as the difference between `firstpayment` and `origination date`\n",
    "# need do it later \n",
    "\n",
    "# 2. Calculate `ADJ Rem month / ORIG_TERM`\n",
    "final_df['adjusted_remaining_ratio'] = final_df['ADJ_REM_MONTHS'] / final_df['ORIG_TERM']\n",
    "\n",
    "# 3. Convert `NUM_BO` (number of borrowers) from string to integer\n",
    "final_df['NUM_BO'] = pd.to_numeric(final_df['NUM_BO'], errors='coerce')\n",
    "\n",
    "# 4. Convert `FIRST_FLAG` (N: 0, Y: 1)\n",
    "final_df['FIRST_FLAG'] = final_df['FIRST_FLAG'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "# 5. Convert `PURPOSE` (P: 0, C/R/U: 1)\n",
    "final_df['PURPOSE'] = final_df['PURPOSE'].map({'P': 0, 'C': 1, 'R': 1, 'U': 1})\n",
    "\n",
    "# 6. Convert `PROP` (SF: 0, PU: 1, CO: 2, MH: 3, CP: 4)\n",
    "prop_map = {'SF': 0, 'PU': 1, 'CO': 2, 'MH': 3, 'CP': 4}\n",
    "final_df['PROP'] = final_df['PROP'].map(prop_map)\n",
    "\n",
    "# 7. Convert `OCCUPANCY_STATUS` (U: 0, P: 1, I: 2, S: 3)\n",
    "occupancy_map = {'U': 0, 'P': 1, 'I': 2, 'S': 3}\n",
    "final_df['OCC_STAT'] = final_df['OCC_STAT'].map(occupancy_map)\n",
    "\n",
    "# 8. Convert `STATE` to integers by alphabetically sorting unique values and assigning them numbers\n",
    "# Provided list of states in the dataset\n",
    "states = ['GA', 'KS', 'IL', 'IN', 'TX', 'UT', 'MO', 'IA', 'OR', 'DE', 'CA', 'MI', 'KY',\n",
    "          'CO', 'NY', 'PA', 'WI', 'WA', 'VA', 'AZ', 'MD', 'TN', 'MA', 'OH', 'SC', 'AK',\n",
    "          'AL', 'LA', 'MN', 'NC', 'AR', 'MS', 'OK', 'NE', 'NJ', 'ID', 'FL', 'ND', 'NV',\n",
    "          'NM', 'CT', 'VT', 'WV', 'DC', 'ME', 'SD', 'NH', 'MT', 'HI', 'PR', 'RI', 'WY',\n",
    "          'VI', 'GU']\n",
    "\n",
    "# Sort states alphabetically and assign rank values starting from 1\n",
    "state_mapping = {state: idx + 1 for idx, state in enumerate(sorted(states))}\n",
    "final_df['STATE'] = final_df['STATE'].map(state_mapping)\n",
    "\n",
    "\n",
    "# 9. Leave `MSA` and `ZIP` columns as-is for now, as instructed, need further exploration\n",
    "\n",
    "# 10. Drop the `PRODUCT` column\n",
    "final_df.drop(columns=['PRODUCT'], inplace=True)\n",
    "\n",
    "# 11. Convert `DLQ_STATUS` from string to integer, treating non-numeric as NaN\n",
    "final_df['DLQ_STATUS'] = pd.to_numeric(final_df['DLQ_STATUS'], errors='coerce')\n",
    "\n",
    "# 12. Convert `MOD_FLAG` (N: 0, Y: 1), removing rows where `MOD_FLAG` is NA\n",
    "final_df = final_df.dropna(subset=['MOD_FLAG'])\n",
    "final_df['MOD_FLAG'] = final_df['MOD_FLAG'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "# 13. Convert `SERV_IND` (N: 0, Y: 1)\n",
    "final_df['SERV_IND'] = final_df['SERV_IND'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "# 14. Convert `HOMEREADY_PROGRAM_INDICATOR` (7: 0, F: 1, H: 2)\n",
    "final_df['HOMEREADY_PROGRAM_INDICATOR'] = final_df['HOMEREADY_PROGRAM_INDICATOR'].map({'7': 0, 'F': 1, 'H': 2})\n",
    "\n",
    "# 15. Convert `RELOCATION_MORTGAGE_INDICATOR` (N: 0, Y: 1)\n",
    "final_df['RELOCATION_MORTGAGE_INDICATOR'] = final_df['RELOCATION_MORTGAGE_INDICATOR'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "# 16. Convert `HIGH_BALANCE_LOAN_INDICATOR` (N: 0, Y: 1)\n",
    "final_df['HIGH_BALANCE_LOAN_INDICATOR'] = final_df['HIGH_BALANCE_LOAN_INDICATOR'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PPMT_FLG'] = df['PPMT_FLG'].map({'N': 0, 'Y': 1})\n",
    "print(df['interst_only_loan'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prepayment_penalty'] = df['PPMT_FLG'].map({'N': 0, 'Y': 1})\n",
    "print(df['prepayment_penalty'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['interst_only_loan'] = df['IO'].map({'N': 0, 'Y': 1})\n",
    "print(df['interst_only_loan'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17 HIGH_LOAN_TO_VALUE_HLTV_REFINANCE_OPTION_INDICATOR\n",
    "final_df['HIGH_LOAN_TO_VALUE_HLTV_REFINANCE_OPTION_INDICATOR'] = final_df['HIGH_LOAN_TO_VALUE_HLTV_REFINANCE_OPTION_INDICATOR'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "#18 PAYMENT_DEFERRAL_MOD_EVENT_FLAG\n",
    "final_df['PAYMENT_DEFERRAL_MOD_EVENT_FLAG'] = final_df['PAYMENT_DEFERRAL_MOD_EVENT_FLAG'].map({'N': 0, 'Y': 1,'7': 2})\n",
    "# Checking the data types and first few rows to confirm transformations\n",
    "\n",
    "#19 PPMT_FLG\n",
    "final_df['PPMT_FLG'] = final_df['PPMT_FLG'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "#20 IO\n",
    "final_df['IO'] = final_df['IO'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "# Map classification categories to integers (Bank: 0, Mortgage Company: 1, Investor: 2, Other: 3)\n",
    "seller_type_mapping = {'Bank': 0, 'Mortgage Company': 1, 'Other': 2, 'Investor': 3}\n",
    "final_df['seller_type'] = final_df['seller_type'].map(seller_type_mapping)\n",
    "\n",
    "\n",
    "Service_type_mapping = {'Bank': 0, 'Mortgage Company': 1, 'Other': 2}\n",
    "final_df['servicer_type'] = final_df['servicer_type'].map(Service_type_mapping)\n",
    "\n",
    "\n",
    "\n",
    "print(final_df.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NaN values in each column in the dataset\n",
    "nan_counts = final_df.isna().sum()\n",
    "\n",
    "# Display columns with NaN values and their counts\n",
    "nan_counts[nan_counts > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in the specified columns and save the cleaned DataFrame to a CSV file\n",
    "\n",
    "# Specify columns with NaN values to drop\n",
    "columns_with_nan = [\n",
    "    'SERVICER', 'LOAN_AGE', 'REM_MONTHS', 'ADJ_REM_MONTHS', 'MATR_DT',\n",
    "    'DTI', 'CSCORE_B', 'ZIP', 'PPMT_FLG', 'IO', 'DLQ_STATUS', \n",
    "    'SERV_IND', 'adjusted_remaining_ratio'\n",
    "]\n",
    "\n",
    "# Drop rows with NaN values in these columns\n",
    "cleaned_df = final_df.dropna(subset=columns_with_nan)\n",
    "final_df = final_df.drop(columns=['SELLER','SERVICER' 'ORIG_DATE','FIRST_PAY','MSA','ZIP'], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"../data/final_16Q1.csv\"\n",
    "final_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hltv_INDICATOR'] = df['HIGH_LOAN_TO_VALUE_HLTV_REFINANCE_OPTION_INDICATOR'].map({'N': 0, 'Y': 1})\n",
    "print(df['hltv_INDICATOR'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MGT6785",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
